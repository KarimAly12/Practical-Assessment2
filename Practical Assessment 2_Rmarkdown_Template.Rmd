---
title: "Data Wrangling"
author: "Student name submitting the assessment report comes here"
subtitle: Practical assessment 2
date: ""
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---


## **Setup**

```{r}

# Load the necessary packages required to reproduce the report. For example:

library(dplyr)
library(kableExtra)
library(magrittr)
library(readr)


```


## **Student names, numbers and percentage of contributions**
```{r, echo=FALSE}

# Add your names, numbers and percentage of your contribution here.

na<- c(" Karim Aly"," Bhanuchandu Bochu")
no<- c(" 4104031","  4120371")
pc<- c("Percentage1","  Percentage2")

s<- data.frame(cbind(na,no,pc))
colnames(s)<- c("Student name", "Student number", "Percentage of contribution")

s %>% kbl(caption = "Group information") %>%
  kable_classic(full_width = F, html_font = "Cambria")

```
<br>
<br>

## **Executive Summary**

In this project, we integrated two datasets to create a unified dataset for comprehensive analysis. The primary objective was to prepare the data for further analytical tasks. The process began by importing the datasets and eliminating irrelevant columns to streamline the data. We then merged the datasets into a single, cohesive dataset.

To better understand the data, various R scripts were used for exploratory analysis, providing insights into the data's structure and attributes. We identified untidy data and applied necessary steps to reshape it into a tidy format, ensuring consistency and ease of analysis. Additionally, we created a new variable derived from two existing variables to enrich the dataset with more relevant information.

Next, we scanned the dataset for any missing values, special values, inconsistencies, and outliers. These issues were addressed using appropriate techniques, such as imputation for missing data and transformations to correct inconsistencies or outlier effects. As a final step, a transformation was applied to one of the variables to improve the data's suitability for analysis.



<br>
<br>

## **Data**

gdp of puerto rico 2017 = 31108


```{r}

# Import the data, provide your R codes here.

# Import the data without column names
us_gdp_by_state_df <- read_csv("us-gdp-by-state.csv", show_col_types = FALSE)

#aus_real_estate_df$City <- toupper(aus_real_estate_df$City)

us_gdp_by_state_df <- us_gdp_by_state_df %>% filter(Area != "United States")
us_gdp_by_state_df <- us_gdp_by_state_df %>% select(c("Area", "2017"))

us_gdp_by_state_df <- us_gdp_by_state_df %>% add_row(Area = "Puerto Rico", `2017` = 31108)



# Display the first few rows of the data frame
print(head(us_gdp_by_state_df))



```

```{r}
us_realtor_df <- read.csv("us-realtor-data.csv", stringsAsFactors = TRUE)



print(tail(us_realtor_df))

```

```{r}
merged_df = merged_df <- left_join(us_realtor_df, us_gdp_by_state_df, by = c("state" = "Area"))

print(tail(merged_df))
```



Data Import and Filtering:

The script starts by importing a CSV file named "us-gdp-by-state.csv" into a data frame called us_gdp_by_state_df, with the column names not being shown initially. The data represents GDP information by state in the United States.
The data frame is then filtered to exclude any rows where the "Area" column equals "United States", as the analysis is intended to be at the state level rather than the entire country.
The script selects only the "Area" and "2017" columns, which presumably represent the state names and their corresponding GDP for the year 2017.

Another dataset, "us-realtor-data.csv", is imported into a data frame called us_realtor_df. This dataset is assumed to contain real estate information by state.

Merging Data:

Finally, a left join is performed between us_realtor_df and us_gdp_by_state_df based on the state/area names. The resulting data frame, merged_df, includes the real estate data along with the GDP information for each state (or area). This step integrates the GDP data into the real estate dataset to allow for analysis combining both economic and real estate factors.

<br>
<br>

## **Understand** 

```{r}
print(str(merged_df))

```

```{r}
#merged_df$status <- as.factor(merged_df$status)
#levels(merged_df$status) <- c("ready_to_build", "for_sale","sold")

merged_df$prev_sold_date <- as.Date(merged_df$prev_sold_date, format = "%d/%m/%Y")

print(str(merged_df))

```

```{r}

# This is the R chunk for the Understand Section

```

Provide explanations here. 

<br>
<br>

##	**Tidy & Manipulate Data I **

In the merged dataset, some column headers are actual values rather than variable names. Specifically, the "2017" column should be converted to a variable representing the year, with its values placed under a new "year" column, while the corresponding GDP figures should be stored in a separate "GDP" column. This transformation restructures the data for proper analysis by organizing each yearâ€™s GDP in a more suitable long format.

```{r}

# This is the R chunk for the Tidy & Manipulate Data I 
merged_df <- merged_df %>% gather(`2017`, key = "year", value = "GDP")

tail(merged_df)

```

This R code reshapes the merged_df data frame by converting the 2017 column into a key-value pair, where "year" becomes the new column for the year and "GDP" for the corresponding GDP values. This transformation changes the data from a wide to a long format, making it more suitable for time series analysis.


Provide explanations here. 

<br>
<br>

## **Tidy & Manipulate Data II** 

```{r}

###Question NO 6


# Create a new variable 'price_per_sqft' using base R
merged_df$price_per_sqft <- merged_df$price / merged_df$house_size

# Display the first few rows of the updated dataframe
print(head(merged_df))

# Summary statistics of the new variable
summary(merged_df$price_per_sqft)

print(tail(merged_df))

# This is the R chunk for the Tidy & Manipulate Data II 

```

The code adds a new variable called `price_per_sqft` to the `merged_df` data frame. This new variable is calculated by dividing the house price (`price`) by the house size (`house_size`) for each row, giving the price per square foot of each property. The script then displays the first few rows of the updated data frame to confirm the new column was added correctly.

After that, it generates summary statistics for the `price_per_sqft` variable, providing insights such as the minimum, median, mean, and maximum values. Lastly, the script shows the last few rows of the modified data frame for further inspection.
Provide explanations here. 

<br>
<br>

##	**Scan I **


```{r}

#install.packages("Hmisc")
library(Hmisc)


```

```{r}
print(colSums(is.na(merged_df)))
# This is the R chunk for the Scan

```

```{r}

is.nan(merged_df)
```


```{r}
merged_df <- merged_df[complete.cases(merged_df[, c("brokered_by", "price", "street", "city", "zip_code", "GDP")]), ]
print(colSums(is.na(merged_df)))

```

```{r}
merged_df <- merged_df %>%
  group_by(city, state) %>%
   mutate(bed = ifelse( is.na(bed), 
                        mean(bed[!is.na(bed)], na.rm = TRUE), 
                        bed)) %>% 
ungroup()

merged_df <- merged_df %>%
  filter(!is.na(bed))


print(colSums(is.na(merged_df)))


```

```{r}
merged_df <- merged_df %>%
  group_by(city, state) %>%
   mutate(bath = ifelse( is.na(bath), 
                        mean(bath[!is.na(bath)], na.rm = TRUE), 
                        bath)) %>% 
ungroup()

merged_df <- merged_df %>%
  filter(!is.na(bath))

print(colSums(is.na(merged_df)))



```

```{r}
merged_df <- merged_df %>%
  group_by(city, state) %>%
   mutate(house_size = ifelse( is.na(house_size), 
                        mean(house_size[!is.na(house_size)], na.rm = TRUE), 
                        house_size)) %>% 
ungroup()

merged_df <- merged_df %>%
  filter(!is.na(house_size))

print(colSums(is.na(merged_df)))



```

```{r}
merged_df <- merged_df %>%
  group_by(city, state) %>%
   mutate(price_per_sqft = ifelse( is.na(price_per_sqft), 
                       price/house_size, 
                        price_per_sqft)) %>% 
ungroup()

#merged_df <- merged_df %>%
#  filter(!is.na(price_per_sqft))

print(colSums(is.na(merged_df)))



```





```{r}
#install.packages("editrules")
library(editrules)

#Define the rule
Rule1 <- editset(c("price > 0"))


# Find the violated edits
violations <- violatedEdits(Rule1, merged_df)


# Convert the violations matrix to a logical vector
violation_vector <- apply(violations, 1, any)

# Print the records that violated the rule
violated_records <- merged_df[violation_vector, ]

print(violated_records)

merged_df <- merged_df %>% filter(price != 0)


```


```{r}


#Define the rule
Rule2 <- editset(c("bed > 0"))


# Find the violated edits
violations <- violatedEdits(Rule2, merged_df)


# Convert the violations matrix to a logical vector
violation_vector <- apply(violations, 1, any)

# Print the records that violated the rule
violated_records <- merged_df[violation_vector, ]

print(violated_records)
```
```{r}


#Define the rule
Rule2 <- editset(c("bed > 0"))


# Find the violated edits
violations <- violatedEdits(Rule2, merged_df)


# Convert the violations matrix to a logical vector
violation_vector <- apply(violations, 1, any)

# Print the records that violated the rule
violated_records <- merged_df[violation_vector, ]

print(violated_records)
```

```{r}


#Define the rule
Rule4 <- editset(c("house_size > 0"))


# Find the violated edits
violations <- violatedEdits(Rule4, merged_df)


# Convert the violations matrix to a logical vector
violation_vector <- apply(violations, 1, any)

# Print the records that violated the rule
violated_records <- merged_df[violation_vector, ]

print(violated_records)
```
```{r}


#Define the rule
Rule5 <- editset(c("GDP > 0"))


# Find the violated edits
violations <- violatedEdits(Rule5, merged_df)


# Convert the violations matrix to a logical vector
violation_vector <- apply(violations, 1, any)

# Print the records that violated the rule
violated_records <- merged_df[violation_vector, ]

print(violated_records)
```

Provide explanations here. 

<br>
<br>

##	**Scan II**

```{r}
# Filter the dataset to include only Alabama
alabama_df <- merged_df[merged_df$state == "Alabama", ]

# Display the first few rows of the Alabama dataset
print(head(alabama_df))

# Display the dimensions of the Alabama dataset
print(dim(alabama_df))
```

```{r}

#Question 8

# This is the R chunk for the Scan II
detect_outliers <- function(x) {
  if(!is.numeric(x)) return(NA)
  Q1 <- quantile(x, 0.25, na.rm = TRUE)
  Q3 <- quantile(x, 0.75, na.rm = TRUE)
  IQR <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR
  upper_bound <- Q3 + 1.5 * IQR
  return(sum(x < lower_bound | x > upper_bound, na.rm = TRUE))
}

# Function to check for special values
check_special_values <- function(x) {
  if(is.numeric(x)) {
    return(c(sum(x == 0, na.rm = TRUE), sum(x < 0, na.rm = TRUE)))
  } else {
    return(c(NA, NA))
  }
}

# Check for missing values
missing_values <- colSums(is.na(alabama_df))
print("Missing values per column:")
print(missing_values)

# Check for potential special values
special_values <- sapply(alabama_df, check_special_values)
rownames(special_values) <- c("Zero_Count", "Negative_Count")
print("Potential special values:")
print(special_values)

# Detect outliers in each numeric column
outliers_count <- sapply(alabama_df, detect_outliers)
print("Number of outliers in each numeric column:")
print(outliers_count)

# Calculate percentage of outliers
outliers_percentage <- (outliers_count / nrow(alabama_df)) * 100
print("Percentage of outliers in each numeric column:")
print(outliers_percentage)

# Visualize distribution of a few key numeric variables
par(mfrow = c(2, 2))
hist(alabama_df$price, main = "Distribution of Price", xlab = "Price")
hist(alabama_df$house_size, main = "Distribution of House Size", xlab = "House Size")
hist(alabama_df$price_per_sqft, main = "Distribution of Price per Sq Ft", xlab = "Price per Sq Ft")
hist(as.numeric(alabama_df$prev_sold_date), main = "Distribution of Previous Sold Date", xlab = "Date")
par(mfrow = c(1, 1))

# Box plots for key numeric variables
boxplot(alabama_df$price, main = "Box Plot of Price")
boxplot(alabama_df$house_size, main = "Box Plot of House Size")
boxplot(alabama_df$price_per_sqft, main = "Box Plot of Price per Sq Ft")
```

Provide explanations here. 

<br>
<br>

##	**Transform **

```{r}
#question 9


# This is the R chunk for the Transform Section
library(dplyr)
library(moments)  # for skewness calculation

# Assuming 'merged_df' contains the variable 'price_per_sqft'
# We will apply a log transformation to 'price_per_sqft' to reduce skewness

# Check skewness before transformation
skewness_before <- skewness(merged_df$price_per_sqft, na.rm = TRUE)
print(paste("Skewness before log transformation:", skewness_before))

# Apply log transformation
merged_df$log_price_per_sqft <- log(merged_df$price_per_sqft)

# Check skewness after transformation
skewness_after <- skewness(merged_df$log_price_per_sqft, na.rm = TRUE)
print(paste("Skewness after log transformation:", skewness_after))

# Visualizing the distribution before and after transformation
par(mfrow = c(1, 2))
hist(merged_df$price_per_sqft, main = "Price per Sqft (Before)", xlab = "Price per Sqft", col = "blue")
hist(merged_df$log_price_per_sqft, main = "Log-Transformed Price per Sqft", xlab = "Log(Price per Sqft)", col = "green")
par(mfrow = c(1, 1))
```

Provide explanations here. 


<br>
<br>

##	**Presentation **

Add the link to your presentation inside the brackets below.

[Presentation](...)

